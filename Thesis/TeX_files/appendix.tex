\appendix
\chapter{More Data probably}
\chapter{Background Neural Network}
	\section{Introduction to Neural Networks}
	Glossary: \label{Glossary}
	\begin{itemize}
		\item Network: A series of layers. The first layer of a network is called the input layer, the last layer is called the output layer. Any layers in between are called hidden layers. The amount of hidden layers is called the \textbf{depth} of the network.\\
		\item Layer: A collection of neurons. The amount of neurons in a layer is called the \textbf{width} of a layer.\\
		\item Neuron: A single node in a layer. It contains a single number formed by a weighted sum of it's inputs evaluated by the activation function.\\
		\item Activation function: A non-function applied to the weighted sum of a neuron. Used to introduce non linearity into the network in order to enable non linear model "fitting". %\todo{improve wording} \\
		\item Weight: Each connection between layers has it's own weight factor. These are adjusted during training to fit the data. Weights are often referred to as parameters.\\
		\item Regularization: Methods used to suppress overfitting. \\%\todo{recheck wording} \\
		\item Metric: Additional information gathered during training/testing. \\
		\item Loss function: Function that dictates the optimization, e.g. Root Mean Squared. \\
		\item Training Data: Set of Data used during training phase. Weights are adjusted to these data. \\
		\item Validation Data: Set of Data used during training phase to evaluate training results intermediary. \\
		\item Test Data: Data set not seen during training to evaluate trained network performance. \\
		\item Input: Data fed to the network for training or evaluation.
		\item Output: Prediction of the network.
		\item Label: True output value for input data.
		\item Hyperparameter: A parameter not changed during training e.g. width and depth of the network.
	\end{itemize}
	\section{Examples of neural networks in different contexts}
\chapter{Background Gaussian Processes}
	\section{Baysian Statistics}