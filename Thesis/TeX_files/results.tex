\chapter{Results}
	\label{Chap:Results}
	\section{Physical Results}
	\section{Neural Networks}
		The following subsets of hyper parameters have been investigated:
		\subsection{Architecture}
			To begin with a general analysis of the architecture is performed. All architectures tested in this section, have been tested with the maximum amount of data to ensure the best possible learning process. Furthermore they have been validated and tested using $2^{15}$ data points each. Due to assumptions of best suited hyper parameters as explained in chapter \ref{Chap:Methods} section \ref{HyperPar}, the initial layer of neurons uses a sigmoid activation function followed by LeakyReLU layers and the output layer has a linear activation function. Dropout has been set to 0.4, the loss function is Root-Mean-Squared and the optimizer is Adam with NAG-momentum and decay. All of these parameters can be found in table \ref{Tab:ArchPar}.\\
			
			\begin{tabular}{c|c|c}
				Parameter & Value & Note\\
				\hline
				Number of Training Points & $2^{17}$ & Sobol Points to ensure coverage of parameterspace\\
				Number of Validation Points & $2^{15}$ & Points Randomly sampled from Parameterspace \\
				Number of Test Points & $2^{15}$ & Randomly sampled from Parameterspace\\
				Optimizer & Adam & \\
				Learning Rate & 0.01 & Starting Value \\
				Momentum & & \\
				Decay & & \\
				Loss Function & RMS & \\
				Activation function & Leaky ReLU & $\alpha = 0.01$\\
			\end{tabular}
		
			\begin{tabular}{c|c}
				Shape & Test Loss & Note \\
				\hline
				Evenly spaced & & \\
				& & \\
				Triangular & & \\
				Reversed Triangle & & \\
				Hourglass & & \\
				Reversed Hourglass & & \\
				Stretched Reversed Hourglass& & \\
			\end{tabular}
		
			With the general structure locked into place the next test series is used to investigate how the size of the network impacts performance, based on the assumption that a larger network will have better adaptation and hence precision, but takes more time to train, evaluate and make predictions. At first the depth of the network is kept the same while the width is varied, afterwards the width is kept constant. Lastly a test series with about constant amount of weights is done. The Results can be found in tables \ref{Tab:CDepth}, \ref{Tab:CWidth}, \ref{Tab:CPara}.
			
			\begin{tabular}{c|c}
				Depth & Test Loss\\
				\hline
			\end{tabular}
			\begin{tabular}{c|c}
				Width & Test Loss\\
				\hline
			\end{tabular}
			\begin{tabular}{c|c|c}
				Number of Total Parameters & Depth & Width & Test Loss \\
				\hline
			\end{tabular}
			\paragraph{Conclusion} After these tests we can conclude that for the given datastructure and problem the most optimal structure is: \todo{Summarize final architecture}.

		\subsection{Training Set Size}
			Depth and Width together determine the total number parameters and complexity of the network. The question is how complex does the network needs to be to accurately learn the model. Ideally we'd like to trim the network as much as possible without loosing too much accuracy.\\
			\begin{tabular}{c|c|c}
				Number of Training Points & Number of Validation Points & Number of Test Points\\
				\hline
			\end{tabular}
			\subsubsection{Activation}
				\begin{itemize}
					\item ReLU \\
					\item PReLU \\
					\item ELU \\
					\item Conclusion
				\end{itemize}
			\subsubsection{Optimizer}
				\begin{itemize}
					\item SGD with momentum\\
					\item Adam 
				\end{itemize}
		\subsection{Derivatives}
%	\section{Gaussian Processes}
%		\subsection{Subdivion of parameter space}
%		\subsection{Derivatives}
%	\section{Comparison}
%		\subsection{Accuracy}
%	\section{NNGP - Maybe}