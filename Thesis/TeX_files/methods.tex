\chapter{Methods}
	This chapter is concerned with introducing the methods used to investigate the data reduction of the previously introduced model. The focus of this work will be on artificial neural networks (ANN in short) and Gaussian processes.
	The following list will provide a brief overview of other machine learning techniques that could be employed to 
	\begin{itemize}
		\item Support Vector Machines (SVM):\\
		%Support Vector Machines
		Support Vector Machines (in short SVM) are used to classify data similar to ANNs. In contrast to ANNs SVMs are build up from theory and contain little Hyperparameters\footnote{Please refer to section \ref{HyperPar} for further information.} making them easier to analyse and less prone to overfitting. Generally speaking a SVM tries to seperate data by calculating a hyperplane using given training data. In more complex situations transformations of the parameter space into a higher dimensional space wherein the p
		\item Random Forests:\\
		\todo{Short description of Random Forests}
		\item Boosting:\\
		\todo{Short description of Boosting methods}
	\end{itemize}
	\section{Neural Networks}
		The following section is concerned with discussing neural networks as a means of investigating functional dependencies.\footnote{To aid with understanding the terminology used there is a glossary in the appendix section \ref{Glossary}.}\\		
		\subsection{General Introduction To Neural Networks}
		%Definition
		An artificial neural network (ANN) in the following called neural network, abbreviated to NN, is "a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs." \cite{NNPrimer} %\todo{cite: In "Neural Network Primer: Part I" by Maureen Caudill, AI Expert, Feb. 1989}\\
		% Historical Context, First Introduction, Basic idea
		First concepts of learning algorithms based on neural plasticity have been introduced in the late 1940s by D. O. Hebb \todo{citation needed}. In 1975 backpropagation became possible via an algorithm by Werbo\todo{citation needed}, this led to an increase in machine learning popularity. During the 1980s other methods like support vector machines and linear classifiers became the preferred and/or dominating machine learning approach. With the rise in computational power in recent years neural networks have gained back a lot of popularity.\\
		The concept idea of neural networks is to replicate the ability of the human brain to learn and to adapt to new information. The structure and naming convention reflect this origin.\\
		%Structue: Requirements, Deep networks
		A neural network is made up of small processing units called neurons. These are grouped together into so called layers. Every network needs at least two layers, the input layer and the output layer. If a network has intermediary layers between input and output, they are called hidden layers. A network with at least two hidden layers is called a deep neural network (DNN). The amount of layers in a network is called the depth of the network. While the amount of neurons in a layer is called the layers width.
		%As mentioned above the neurons are highly interconnected. The structure of connections determine the type of network. In the following we will discuss fully connected networks\footnote{Fully connected networks are also called dense networks.}. These feature a connection between each neuron of adjacent layers\footnote{The amount of connections between two layers in a dense network is therefore equal the the product of the width of adjacent layers.}
		%Basic working principle, introduced non-linearity
		In a typical NN information stored in neurons is transferred into the next layer by a weighted sum. The connected neuron of the following layer then applies a non-linear function, called activation function, to calculate it's final value. This process in repeated until the output layer is reached. The activation function as well as the amount and order\todo{reword "order"} of connections can vary in between layers. The system according to which a network is designed is called a network architecture. The most important architectures in the following work will be \textit{dense deep feed forward} and \textit{autoencoder}. To give insight into the basic working principle an example neural network is depicted and described in the following section \ref{NNExample}.\\
		~\\
		%\todo{General description of NN use-cases and strength compared to "old" machine learning methods such as support vector manchines.}
		%Generally speaking neural networks are used to solve the equation $f(x) = y$ for $f()$. In other words it is used to make a fit to data points. There are many already established well known methods to do so. Therefore it is natural to ask what the advantages and disadvantages of neural networks compared to more traditional fitting methods are. To contrast neural networks we will consider support vector machines and analytical fits.
		Neural networks are usually used in two ways, optimization or classification. Well known examples are handwriting recognition as classification and least mean squares (LMS) optimization\todo{think of better example}.
		\subsection{Fully Connected}
			\label{NNExample}
			A fully connected or dense neural network like depicted in figure \ref{Img_NNFully} is characterized by connecting every neuron from the previous to the following layer with a weight. In contrast to other networks this allows for a very high flexibility but also lacks the spatial context of data. This kind of network is especially well suited for data that is given in form of vectors or drawn from an arbitrary parameter space.%\todo{insert reference}
			
			\begin{figure}
				\includegraphics[width=\textwidth]{images/simpleNN.png}
				\caption{Schematic structure of the most basic fully connected deep neural network. Indicated are the input (yellow), output (red) and hidden layers (blue and green). Each neuron outputs to all neurons in the following layer, but there are no interconnection between neurons of the same layer. Note that while the network has the minimum depth (2 hidden layers) to qualify for a deep neural network, the width  could be smaller.}
				\label{Img_NNFully}
			\end{figure}

			%\todo{Funktionsweise}
			The working principle is to form a weighted sum $\sum_{k=1}^{N} w_{j,k} \cdot x_{k}$ over the values from neurons of the previous layer $x_{k}$ weighted by the connecting weight $w_{j,k}$. The weighted sum is then evaluated by the activation function $\sigma()$ such that the new value $x_j = \sigma(\sum_{k=1}^{N} w_{j,k} \cdot x_{k})$. Here k refers to the index of the neuron in the previous layer and j to the index of the neuron in the current layer.\footnote{The order of indices becomes more intuitive when talking about backpropagation and it's matrix notation in section \ref{BackProp}}\\
			Since forming a weighted sum is a linear operation the activation function must be non-linear to enable the network to learn non-linear behaviour.\todo{ref needed} The most common activation functions are the rectifier also called rectified linear unit (ReLU) and exponential linear unit (ELU) shown in figure \ref{ReLUELU}. Both are inspired by the asymmetrical behaviour of biological neural connections\todo{reference needed}.\\
		
		\subsection{Training}
			%\todo{Describe training cycle}
			Before a neural network can be put to work it needs to be trained. To train a NN a set of training and test data has to be generated. This work uses the afore mentioned monte carlo simulations from the EIRENE code.
			The training data consists of input e.g. temperature and density of the plasma and output e.g. the sputtering rate of the first wall. The EIRENE input data is used as input of the network and the sputtering rate is compared to the output of the network via a cost function. Afterwards the weights of the network are adjusted by using backpropagation, which is a method that calculates partial weight derivatives of the output. A more detailed explanation can be found in section \ref{BackProp} \todo{add citation}.\\
			%\todo{Explain use of batches, Epochs, Regularization}
			
			~\\
			\subsubsection{Backpropagation}
				\label{BackProp}
				\begin{equation}
					\frac{\partial F}{\partial w}
				\end{equation}
			
		\subsection{Activation Functions}
			\todo{Write Transition}
			When designing a neural network it is important to consider which activation function to use. There are requirements of a suited activation as well as varying advantage of using one or another. A major problem of neural networks can be that without a zero centred data set vanishing of gradients can occur that limits or even stops the learning process. To better understand this phenomenon it is necessary to look at the backpropagation in the training process.\\
			As explained in section \ref{BackProp} in order to adjust the weights it is necessary to calculate the partial derivative of the cost function in respect to each weight. These derivatives in general depend on the derivative of the previous layer. Choosing an activation like a sigmoid \footnote{$f(x)=\frac{1}{1+e^-x}$} leads to a derivative \footnote{$f(x)=\frac{e^-x}{(e^-x+1)^2$}} with vanishing values at the fringes. For each layer between the current and the output the backpropagation will have a partial derivative as a factor with values between 0 and 1. Hence in a network with realistic depth e.g. 50, the partial derivative calculated for adjusting the weight will be almost always negligible for the beginning layers.\\
			
			%\begin{equaition}
			%	\frac{\partial L}{\partial a} = \frac{\partial L}{\partial d} * \frac{\partial d}{\partial c} * \frac{\partial c}{\partial b} * \frac{\partial b}{\partial a}
			%\end{equaition}
			
			To avoid vanishing gradients a better choice of activation can be found. The most common activation for neural networks is the rectified linear unit (ReLU).
			\begin{equation}
				f(x) = 	\begin{cases}
							0 & x < 0\\
							x & x \geq 0
						\end{cases}
				\label{EQ:ReLU}
			\end{equation}
			The derivatives of this function is easily computed to 0 for $ x < 0$ and 1 for $ x \geq 0$. While an activation like the sigmoid function has two sided saturation\footnote{Values at either end of the spectrum have small derivatives.}\todo{reword} which lead to vanishing gradients. The ReLU activation saturates for negative values, which can be interpreted as neurons that work like switches specialising in detecting certain features \todo{add citation Vanishing gradients blogpost}. In some networks this is a wanted quality of the ReLU activation.\\
			\todo{Read up on reasoning for this feature}Furthermore one sided saturation 
			
			\begin{figure}
				\begin{subfigure}{.49\textwidth}
					\centering
					\includegraphics[width=0.8*\linewidth]{images/ReLU.pdf}
					\subcaption{Rectified Linear Unit}
					\label{ReLU}
				\end{subfigure}
				\begin{subfigure}{.49\textheight}
					\centering
					\includegraphics{width=.8\linewidth}{images/Elu.pdf}
					\subcaption{Exponential Linear Unit}
					\label{ELU}
				\end{subfigure}
				\caption{Example activation functions rectified linear unit (a) and  exponential linear unit (b) used to introduce non-linearity into neural networks.}
				\label{ReLUELU}
			\end{figure}
		
			%\todo{Mention Convolutional to contrast}
			In contrast to fully connected networks convolutional networks apply a so called filter to the input. These filters consider the inputs of nearby neurons as well. They are usually used in image recognition and require data that has information stored in patterns, most commonly special patterns as in images. Fig. \ref{Img_NNConv} depicts an exemplary convolutional network for image data and gives indication to its working procedure.
		
			\begin{figure}
				\missingfigure[figwidth=0.5*\textwidth]{Picture depicting exemplary convolutional NN}
				\caption{Exemplary Conv NN}
				\label{Img_NNConv}
			\end{figure}
		

			
		 
		\subsection{Hyper parameters}
			\label{HyperPar}
			A hyper parameter refers to a parameter of the network that is not changed during training. Since these can have substantial influence on the performance of the network they will be explained in the following
			\subsubsection{Depth}
				The depth of the network correlates directly to the amount of chained non-linear functions. Therefore it strongly influences the ability of a NN to learn abstract patterns. The more complicated a pattern is the more depth is "required" to learn the pattern. For example a simple classification between left and right only requires a shallow network, whereas recognizing different brands of car requires a deeper network.
			\subsubsection{Width}
				The width of a layer refers to the amount of neurons in that layer. Often networks are build of layers with the same width in each hidden layer. If that is the case one sometimes speaks of the width of the network which is then equivalent to width of a/each hidden layer.
			\subsubsection{Architectures}
			\subsubsection{Activation or Activation Function}
				As previously discussed the activation function introduces non-linearity to the network. Some activation function will be better suited to model a certain problem than others. If information about the model or pattern to be predicted is known, an activation function close to this will have better performance. For example predicting outcome of a sine function will perform better with exponential activations or uneven polynomial activations than even polynomials.%\todo{citation needed}
			\subsubsection{Loss Function}
				Choosing a loss function determines what criteria the network optimizes for which directly corresponds to which patterns it learns. For prediction one typically chooses a root mean square function. For classification cross entropy loss functions are most common.
			\subsubsection{Batch size}
				The Dataset is divided into subsets called batches which are fed to the current network during training. After each batch the weights are adjusted. Splitting the dataset in this way is advantageous to the computational performance during training. Less memory is used during training and the number of epochs trained is reduced. The flip side of using a batch size smaller than the number of training data is that the gradient for optimization will be worse in comparison to the gradient calculated with the full data set. 
			\subsubsection{Epochs}
				An epoch describes a full training cycle of training, validating and adjusting weights for the entire training data set. If the batch size is smaller than the number of training points then multiple\footnote{Number of batches in one epoch = rounded up $\left(\frac{Amount of Training Data}{Batch Size}\right)$} adjustments are made.%\todo{recheck wording}
			\subsubsection{Metrics}
				Metrics are additional information gained from the network during training and evaluation. Metrics are not hyperparameters since they do not influence the resulting network but are an important source of information for further improving the network structure. For example a secondary loss function can be implemented as a metric to evaluate general optimization of the network in contrast to only the chosen loss quantity. 
			\subsubsection{Regularization}
				%\todo{cite https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/}
				Regularization describes methods used to reduce the generalization error but not the training error. Commonly used regularization methods include L1, L2, Dropout and Early Stopping regularization. L1 and L2 regularization is applied by adding a penalty term to the loss function. This requires initial knowledge of input influences. For example an image with bad resolution might have a larger penalty term applied than an image with high resolution. Dropout regularization and early stopping are used to prevent overfitting. Since the amount of parameters in the network is often on the same order of magnitude as the amount of training data, neural networks are prone to overfitting. Early stopping interrupts the training process as soon as the validation loss stops improving by a user set minimum delta. 
	\section{Gaussian Processes}
		\subsection{General Introduction}
		\subsection{GOGAP algorithm}
%	\section{NNGP ?}