\relax 
\citation{NNPrimer}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Methods}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{Short description of Random Forests}{3}}
\@writefile{tdo}{\contentsline {todo}{Short description of Boosting methods}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neural Networks}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}General Introduction To Neural Networks}{4}}
\@writefile{tdo}{\contentsline {todo}{citation needed}{4}}
\@writefile{tdo}{\contentsline {todo}{citation needed}{4}}
\@writefile{tdo}{\contentsline {todo}{reword "order"}{4}}
\@writefile{tdo}{\contentsline {todo}{think of better example}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schematic structure of the most basic fully connected deep neural network. Indicated are the input (yellow), output (red) and hidden layers (blue and green). Each neuron outputs to all neurons in the following layer, but there are no interconnection between neurons of the same layer. Note that while the network has the minimum depth (2 hidden layers) to qualify for a deep neural network, the width could be smaller.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Img_NNFully}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Fully Connected}{5}}
\newlabel{NNExample}{{2.1.2}{5}}
\@writefile{tdo}{\contentsline {todo}{Figure: Picture depicting ReLU}{6}}
\newlabel{ReLU}{{2.2a}{6}}
\newlabel{sub@ReLU}{{a}{6}}
\@writefile{tdo}{\contentsline {todo}{Figure: Picture depicting ELU}{6}}
\newlabel{ELU}{{2.2b}{6}}
\newlabel{sub@ELU}{{b}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example activation functions rectified linear unit (a) and exponential linear unit (b) used to introduce non-linearity into neural networks.\relax }}{6}}
\newlabel{ReLUELU}{{2.2}{6}}
\@writefile{tdo}{\contentsline {todo}{ref needed}{6}}
\@writefile{tdo}{\contentsline {todo}{reference needed}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Training}{6}}
\@writefile{tdo}{\contentsline {todo}{add citation}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{6}}
\newlabel{BackProp}{{2.1.3}{6}}
\@writefile{tdo}{\contentsline {todo}{Figure: Picture depicting exemplary convolutional NN}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Exemplary Conv NN\relax }}{7}}
\newlabel{Img_NNConv}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Hyper parameters}{7}}
\newlabel{HyperPar}{{2.1.4}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Depth}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Width}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Architectures}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Activation or Activation Function}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Function}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Batch size}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Epochs}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Metrics}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Regularization}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gaussian Processes}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}General Introduction}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}GOGAP algorithm}{9}}
\@setckpt{./TeX_files/methods}{
\setcounter{page}{10}
\setcounter{equation}{1}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{@todonotes@numberoftodonotes}{9}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
}
