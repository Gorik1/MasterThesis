\relax 
\citation{SVMBook}
\citation{KernelTrick}
\citation{RandForest}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chap:Methods}{{3}{13}}
\citation{AdaBoost}
\citation{NNPrimer}
\citation{DOHebb}
\citation{PJWerbos}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Neural Networks}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}General Introduction To Neural Networks}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Functionality}{15}\protected@file@percent }
\newlabel{NNExample}{{3.1.2}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic structure of the most basic fully connected deep neural network. Indicated are the input (yellow), output (red) and hidden layers (blue and green). Each neuron outputs to all neurons in the following layer, but there are no interconnection between neurons of the same layer. Note that while the network has the minimum depth (2 hidden layers) to qualify for a deep neural network, the width could be smaller.\relax }}{16}\protected@file@percent }
\newlabel{Img_NNFully}{{3.1}{16}}
\citation{NNEBook}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Training}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.1}Backpropagation}{17}\protected@file@percent }
\newlabel{BackProp}{{3.1.3.1}{17}}
\newlabel{EQ:Activation}{{3.1}{17}}
\newlabel{EQ:BPPart1}{{3.2}{17}}
\newlabel{EQ:BPPart2}{{3.3}{18}}
\@writefile{toc}{\contentsline {paragraph}{Assumptions of the cost function}{18}\protected@file@percent }
\newlabel{EQ:CostCond1}{{3.4}{18}}
\newlabel{EQ:Cost2}{{3.6}{18}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation algorithm}{18}\protected@file@percent }
\newlabel{EQ:BPDel}{{3.7}{18}}
\citation{NNEBook}
\citation{NNOpti}
\newlabel{EQ:BPMat1}{{3.8}{19}}
\newlabel{EQ:BPMat2}{{3.9}{19}}
\newlabel{EQ:BPdCdw}{{3.10}{19}}
\newlabel{EQ:BPdCdb}{{3.11}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.2}Choice of optimizer}{19}\protected@file@percent }
\newlabel{EQ:GD}{{3.12}{19}}
\citation{NNSGD}
\citation{NNNAG}
\citation{BookMultimediaModeling}
\citation{NNElephant}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.3}Regularization}{21}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Search for further references}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hold out}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{L1 Regularization}{22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{L2 Regularization}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dropout}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data variation}{23}\protected@file@percent }
\citation{VanGrad}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Hyperparameters}{24}\protected@file@percent }
\newlabel{HyperPar}{{3.1.4}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.1}Activation Functions}{24}\protected@file@percent }
\newlabel{Sec:Activation}{{3.1.4.1}{24}}
\newlabel{EQ:ReLU}{{3.25}{24}}
\citation{VanGrad}
\newlabel{Fig:ReLU}{{3.2a}{25}}
\newlabel{sub@Fig:ReLU}{{a}{25}}
\newlabel{Fig:ELU}{{3.2b}{25}}
\newlabel{sub@Fig:ELU}{{b}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Example activation functions rectified linear unit (a) and exponential linear unit (b) used to introduce non-linearity into neural networks.\relax }}{25}\protected@file@percent }
\newlabel{Fig:ReLUELU}{{3.2}{25}}
\newlabel{EQ:PReLU}{{3.26}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.2}Architecture}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Depth}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Width}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dense layers}{26}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{insert reference}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feed Forward Networks}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recurrent networks}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoencoder Networks}{26}\protected@file@percent }
\citation{ImgNNZoo}
\citation{ImgNNZoo}
\citation{NNEBook}
\@writefile{toc}{\contentsline {paragraph}{Other types}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.3}Cost function}{27}\protected@file@percent }
\newlabel{EQ:CostCrossEntropy}{{3.28}{27}}
\@writefile{toc}{\contentsline {paragraph}{Batch size and Epochs}{27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Zoo of Neural network architectures. Image taken from \cite  {ImgNNZoo}\relax }}{28}\protected@file@percent }
\newlabel{Fig:NNZoo}{{3.3}{28}}
\@setckpt{./TeX_files/methods}{
\setcounter{page}{30}
\setcounter{equation}{30}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{27}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{subsubsection}{3}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{@todonotes@numberoftodonotes}{8}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
}
